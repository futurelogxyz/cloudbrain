---
doc_type: weread-highlights-reviews
bookId: "3300084217"
author: 赵传霖
cover: https://cdn.weread.qq.com/weread/cover/63/cpplatform_71bvuqo3mw9kxfbxsdcfw5/t7_cpplatform_71bvuqo3mw9kxfbxsdcfw51705302790.jpg
reviewCount: 12
noteCount: 23
readingStatus: 在读
progress: 53%
totalReadDay: 6
readingTime: 5小时50分钟
readingDate: 2024-02-26
isbn: 9787115628688
category: 科学技术-自然科学
lastReadDate: 2024-03-05

---
# 元数据
> [!abstract] 互联网大厂推荐算法实战
> - ![ 互联网大厂推荐算法实战|200](https://cdn.weread.qq.com/weread/cover/63/cpplatform_71bvuqo3mw9kxfbxsdcfw5/t7_cpplatform_71bvuqo3mw9kxfbxsdcfw51705302790.jpg)
> - 书名： 互联网大厂推荐算法实战
> - 作者： 赵传霖
> - 简介： 《互联网大厂推荐算法实战》介绍了互联网大厂当前采用的一些前沿推荐算法，并梳理了这些算法背后的思想脉络与技术框架。 《互联网大厂推荐算法实战》总计10章，内容涵盖了推荐系统的基础知识、推荐系统中的特征工程、推荐系统中的Embedding、推荐系统的各组成模块（包括召回、粗排、精排与重排）所使用的算法技术、推荐算法实践中经常会遇到的难题以及应对之道（其中涉及多任务推荐、多场景推荐、新用户冷启动、新物料冷启动、评估模型效果、定位并解决问题等），最后还用一章的篇幅介绍了推荐算法工程师在工作、学习、面试时应该采取的做法。 《互联网大厂推荐算法实战》既适合推荐系统、计算广告、个性化搜索领域的从业人员阅读，也适合希望从事互联网算法工作的在校学生阅读。
> - 出版时间 2024-01-01 00:00:00
> - ISBN： 9787115628688
> - 分类： 科学技术-自然科学
> - 出版社： 人民邮电出版社有限公司
> - PC地址：https://weread.qq.com/web/reader/cf5329d0813ab8765g0176ff

# 高亮划线

## 1.4 推广搜的区别与联系


- 📌 推荐搜索（简称“推搜”）是为了留住用户来产生流量，所以要服务的目标比较简单，就是为了给用户提供最佳使用体验。而广告是为了将流量变现，所以要兼顾用户、广告主、平台三方的利益，参与方更多、更复杂，优化起来难度更高。  ^3300084217-11-4878-4983
    - ⏱ 2024-02-26 19:34:46 

- 📌 推搜的目标基本上都是能够即时完成的，比如点击、完成播放等。而广告关注的目标是更深层次的转化。  ^3300084217-11-5012-5058
    - ⏱ 2024-02-26 19:35:15 

- 📌 推广搜都要预测点击率(CTR)、转化率(CVR)之类的指标，但对精度的要求不同。推荐与搜索对预测出的CTR/CVR只要求相对准确性，即它们的预测精度能够将用户最喜欢的物料排在最前面，这就足够了。例如用户喜爱A物料甚于喜爱B物料，如果推荐模型给B物料的打分是0.8，那么推荐模型给A物料的打分是0.81还是0.9均不影响产生将A排在B前面的正确排序。
而广告则不同，由于预测出的CTR/CVR要参与广告费用的计算，很小误差都将带来真金白银的损失，因此广告对预测精度要求绝对准确性。在模型的预测结果出来之后，广告还需要对其修正、校准。  ^3300084217-11-5203-5496
    - ⏱ 2024-02-26 19:34:27 
## 2.2 特征提取

 
 

- 📌 有一种Above Click规则规定，只有位于被点击物料上方的未点击物料，才能被纳入训练数据当成负样本  ^3300084217-15-10437-10488
    - ⏱ 2024-02-27 10:57:13 

- 📌 伪特征值  ^3300084217-15-11306-11310
    - ⏱ 2024-02-27 10:50:39 

- 📌 偏差特征只能通过一个线性层接入模型，而绝不能和其他正常特征一起喂入DNN，如图2-4所示。只有这样接入，才能保证预测时无论伪特征值的取值如何，都不会改变排序结果  ^3300084217-15-11503-11583
    - ⏱ 2024-02-27 10:53:01 
## 2.3 数值特征的处理


- 📌 对原始数据进行开方、取对数等非线性变换，先将原来的长尾分布压缩成接近正态分布，再对变换后的数据进行z-score标准化，这样就能够取得更好的效果  ^3300084217-16-2845-2917
    - ⏱ 2024-02-27 11:06:00 
 
 
## 2.4 类别特征的处理


- 📌 推荐算法作为一类特殊的机器学习算法，特点之一就是它的特征空间主要由高维、稀疏的类别特征构成，类别特征是推荐算法的“一等公民”，享受VIP服务  ^3300084217-17-410-480
    - ⏱ 2024-02-27 11:43:49 

- 📌 为了规避维护映射表的麻烦，大型推荐模型中通常采用特征哈希  ^3300084217-17-6431-6459
    - ⏱ 2024-02-27 15:36:30 
## 3.1 无中生有：推荐算法中的Embedding


- 📌 推荐算法面临的经典问题就是记忆与扩展  ^3300084217-20-479-497
    - ⏱ 2024-02-27 15:40:35 
## 4.3 用户行为序列建模


- 📌 DIN中的Attention需要拿候选物料t当Query，这在召回、粗排这些要求用户、物料解耦建模的场景中是做不到的。这时，可以尝试拿用户行为序列中的最后一个物料当作Query对整个行为序列进行Attention操作。毕竟最后的行为反映用户最近的兴趣，可以用来衡量序列中其他历史物料的重要性。  ^3300084217-27-6370-6544
    - ⏱ 2024-02-29 11:27:11 
## 5.2 向量化召回统一建模框架


- 📌 当前推荐系统中的主流召回算法，向量化召回(Embedding-Based Retrieval，EBR)算法。所谓向量化召回，就是将召回问题建模成向量空间内的近邻搜索问题。  ^3300084217-31-413-498
    - ⏱ 2024-03-01 10:32:56 

- 📌 为了让模型“开眼界、见世面”，领教最不靠谱的(￼)组合，喂入召回模型的负样本主要依靠随机采样生成。特别是在U2I召回场景中，坚决不能（只）拿“曝光未点击”样本作为负样本。  ^3300084217-31-7012-7328
    - ⏱ 2024-03-01 10:41:30 
 

- 📌 需要特别强调的是，Hard Negative并非要替代随机采样得到的Easy Negative，而是作为Easy Negative的补充  ^3300084217-31-9095-9163
    - ⏱ 2024-03-01 10:51:33 

- 📌 如果说排序是特征的艺术，那么召回就是样本的艺术，特别是负样本的艺术。做召回，“负样本为王”，负样本的选择对于算法成败是决定性的  ^3300084217-31-9375-9471
    - ⏱ 2024-03-01 10:52:16 
 

- 📌 召回必须解耦、隔离用户信息与物料信息。
■特征策略上，召回无法使用用户与物料的交叉统计特征（如用户标签与物料标签的重合度）。
■模型结构上，召回不能将用户特征、物料特征一股脑喂进DNN，而是两者必须各自处理。用户子模型只利用用户特征，生成用户向量￼。物料子模型只利用物料特征，生成物料向量￼。只允许最后计算￼或￼时，才产生用户信息与物料信息的唯一一次交叉。  ^3300084217-31-10378-11330
    - ⏱ 2024-03-01 11:19:41 

- 📌 理论与现实都导致召回的负样本主要由热门物料组成，这会导致学习到的用户向量u过度远离热门物料的向量，￼的作用是将u向热门物料适当靠拢，但是它的作用并非鼓励而只是补偿。  ^3300084217-31-22570-22888
    - ⏱ 2024-03-01 11:57:47 
# 读书笔记

## 2.2 特征提取

### 划线评论
- 📌 在特征中引入置信度能够给模型提供额外信息，有助于模型做判断。  ^7512473-7PhU4fviO
    - 💭 如何在模型输入中表示置信度？
    - ⏱ 2024-02-27 10:14:14

### 划线评论
- 📌 将用户行为序列直接喂进模型的做法，优点是简单直接，无须进行过多的特征处理；缺点是提取用户兴趣与CTR建模合为一体，都必须在线上完成。特别是在使用DIN或SIM这种“强大但复杂”的模型提取兴趣时，耗时与“序列长度×候选集规模”成正比。所以，如果我们希望从历史更久远、长度更长的行为序列中提取用户兴趣，或者将其应用于召回、粗排等候选集规模很大的场景，这种做法根本就无法满足在线预测与训练的实时性要求。另外，这种做法提取出来的用户兴趣是抽象的向量，可解释性很弱。  ^7512473-7PhVUEqum
    - 💭 关键在于计算量太大，线上计算资源需求大，计算耗时多，如何解决？
    - ⏱ 2024-02-27 10:42:24

### 划线评论
- 📌 [插图]
公式(2-3)
这是因为DNN的高度非线性使伪特征值与真实特征值产生深度交叉，预测时采用不同的伪特征值将产生完全不同的排序结果。这就使排序结果严重依赖于一个在预测时的未知因素，因此是绝不可用的。
除了位置偏差，YouTube还发现视频年龄（当前时间减去上传时间）也会造成偏差。推荐模型会用视频的各种后验消费指标（比如点击率、人均观看时长等）来衡量物料的受欢迎程度。上传早的视频有足够长的时间来积累人气，所以后验指标更好，模型排名更靠前；反之，晚上传的视频还没有积累起足够好的后验数据，模型排名靠后，不利于新视频的冷启动。为了纠正这一偏差，YouTube在训练时将视频年龄作为偏差特征喂入模型，而在预测时统一设置成0。  ^7512473-7PhX7oQM7
    - 💭 偏差特征：位置偏差，“年龄”偏差
    - ⏱ 2024-02-27 11:00:49
   
## 2.3 数值特征的处理

### 划线评论
- 📌 推荐系统中经常要计算各种比率作为特征，比如点击率、点赞率、购买率、复购率等。计算这些比率时，我们经常遇到的一个问题就是样本太少，导致计算结果不可信。比如对于一件商品，只被曝光了一次并被购买，由此我们计算它的购买率是100%，从而认定它是爆款，应该大力推荐，这显然是站不住脚的  ^7512473-7PhXT4dLD
    - 💭 解决方法可以使用 “威尔逊区间平滑”。
    - ⏱ 2024-02-27 11:12:33

### 划线评论
- 📌 CoEC(Click over Expected Click)代替CTR来衡量物料的受欢迎程度  ^7512473-7PhZNmEWo
    - 💭 前文讲过可以通过将 位置偏差 喂给 模型的线性特征 来消偏， CoEC 是另一种位置消偏方法。
    - ⏱ 2024-02-27 11:41:41
   
## 2.4 类别特征的处理

### 划线评论
- 📌 FTRL  ^7512473-7PieDPwKD
    - 💭 FTRL： Follow The Regularized Leader
    - ⏱ 2024-02-27 15:28:22
   
## 3.1 无中生有：推荐算法中的Embedding

### 划线评论
- 📌 使v和f随主目标一同被随机梯度下降(Stochastic Gradient Descent，SGD)法优化  ^7512473-7PigcltUK
    - 💭 如何使 v 和 f 一同被优化？
    - ⏱ 2024-02-27 15:52:09
   
## 4.3 用户行为序列建模

### 划线评论
- 📌 DIN中的Attention需要拿候选物料t当Query，这在召回、粗排这些要求用户、物料解耦建模的场景中是做不到的  ^7512473-7Pl1nYBqx
    - 💭 召回 和 粗排 中为什么要求 用户、物料 解耦建模？
    - ⏱ 2024-02-29 11:25:59

### 划线评论
- 📌 在原始用户行为序列中，“购买过Macbook”这一历史行为反映的只是单次购买本身。而对行为序列Self-Attention之后，同样的历史行为不仅反映了本次购买，还体现了2天前用户购买过iPhone以及和1天后又购买了iPad。通过与前后历史行为的交叉、关联，一个苹果公司忠实用户的形象跃然纸上。这样，底层提取出的用户兴趣的质量得以提高，顶层的CTR建模自然也就降低了难度。  ^7512473-7Pl1NEeXc
    - 💭 对于广告的 曝光和点击 行为序列 是不是合并到一起建模是较好的，因为曝光和点击行为也是有关联的？
    - ⏱ 2024-02-29 11:32:19
   
## 5.2 向量化召回统一建模框架

### 划线评论
- 📌 最让模型“开眼界、见世面”的负采样方法是在所有物料组成的大库中进行随机采样。但是考虑到推荐系统中的物料成百上千万，大库采样的方案代价太高，更是无法满足在线学习的实时性要求  ^7512473-7PmvdWNja
    - 💭 Easy negative 与 hard negative
    - ⏱ 2024-03-01 10:48:15

### 划线评论
- 📌 排序鼓励交叉，召回要求解耦  ^7512473-7PmwZorvn
    - 💭 核心原因是 排序的候选集较小，而召回的候选集太大
    - ⏱ 2024-03-01 11:15:12
   
## 6.1 粗排

### 划线评论
- 📌 由于候选集至多只到万量级，因此粗排双塔在离线生成物料向量后，无须喂入Faiss建立索引，直接存入内存缓存起来，Item ID当“键”，物料向量当“值”。在线预测时，粗排线性遍历召回返回的候选集，逐一从缓存中取出物料向量，与唯一的用户向量做点积就能得到粗排得分。如果某个新物料的向量离线时没有生成，在线预测时从缓存中取不到，就由“物料塔”实时在线生成，并插入缓存中  ^7512473-7PsWIUYDK
    - 💭 当用户规模足够大，不同用户在不同时间召回的物料不完全一样，那积累一段时间后，需要缓存的粗排的物料向量量级不是也很大吗？
所以不应该是存入内存缓存，而是存入 Redis 之类的缓存更合适～
    - ⏱ 2024-03-05 16:28:10
   
# 本书评论
